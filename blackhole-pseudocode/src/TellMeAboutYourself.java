/*
 * Click nbfs://nbhost/SystemFileSystem/Templates/Licenses/license-default.txt to change this license
 * Click nbfs://nbhost/SystemFileSystem/Templates/Classes/Class.java to edit this template
 */

/**
 *
 * @author venka
 */
public class TellMeAboutYourself {
    
}
/*


1. Hi, This is Venkateshwaran Shanmugham, a senior tech lead at Paytam Payments Bank,
where I hold the responsibility of owning the UPI Reconciliation engine.
3. My day to day activities extends beyond just an individual contributer wherein
I collaborate closely with product managers, and engineers from other teams to gather requirements 
and translate them into detailed technical specifications on confluence.
4. I lead and mentor a team of 5 engineers including myself, Working iteratively in an Agile development environment,
I drive the development process forward by creating sprints and ensuring that each sprint delivers 
tangible progress towards our project goals.
5. I am committed to writing clean, maintainable, and efficient code that adheres to coding standards
and make use AI to code on day to day basis using github copilot.
6. I conduct thorough testing and debugging to identify and resolve issues and use github copilot to
generate templates for most of the test cases. 
7. Additionally, I conduct comprehensive code reviews, providing constructive
feedback to my team members to foster a culture of continuous improvement and knowledge sharing.
8. As a senior member of the team, I provide technical guidance and mentorship to team members, 
helping them grow their skills and navigate complex technical challenges.
9. I excel at troubleshooting and debugging where in i was able to identify root cause of a concurrency issue
caused by simple date format, and implementing the solutions.i was also able to reproduce the issue locally 
and check if the solution was able to solve it locally as well.
10. To stay ahead of the curve, I remain up-to-date with the latest industry trends, leveraging AI to automate 
a lot of activities which were earlier done manually.


percentage:
40% coding architectural enhancements, debugging and fixing production issues, 
20% planning deployments, release branch code review and delpoying it
20% code reviews
10% production data and alerts monitoring
10% metrics for stake holders, sprints etc

reconcilation engine
sources:
NPCI files, cbs files, switch files(optimization for merchants async)
in memory reconcilation
parking
discrpancy and reports generation
reset
monitoring and troube shooting
fixing concurrency issues, adding transactional and semaphores wherever necessary
datadog alerts, graphana
parallel switch file generation, parititioned across 4 db clusters

scale issues due to memory issues, parking data didnt fit in.

pod restarts and supports
initially increased memory to see if it helps
had to partition and execute in memory reconcilation on multiple nodes
patitioner and aggreagtor were colocated with one of the nodes


Alternatively
we can use NIFI, kafka partitions, spark, hbase to solve this as well
cassandra provides partitioning across db cluster with leaderless replication, and tunable consistency


All about Generative AI:

Architectures:
GAN - generative adversal networks
generator takes random noise as input to generate samples that resemble training data.
convolution and deconvolution to transform input to output
discreminators classify between real and fake samples.
training
generator tries to produce data that is indistinguishable from real data,
discriminator aims to correctly identify fake and real. 
both are trained simultaneously, trying to fololing and indentifying each other.

VAE - variational autoencoders
encoder
decoder
latent space
training

Transformer - 
enoder-decoder structure
self attention
positioning encoding
training

evaluation metrics
conditional generation
transfer learning and fine tuning
bigGAN, styleGAN and GPT - generative pretrained transformer


pretrained
language models
image generators
variational auto encoders VAE
music generators

advantages of pretrained
reduces training time
improved perfomance
transfer learning


Using pretrrained model
data collection and preprocessing
    clean text, remove noise, stop words, tokenization, normalize(tolowercase, remove stop words), stemming 
    and lemmaization, entity recognition
    extract relevent features like, issue desc, comments, meta data, 
    data labelling, case categories, component,isssue severity etc
    for kbs, jiras, feature extraction which include word embedings, TF-IDF vectors or contexualized 
    embeddings generated by pretrained models
model selection
    natural language understanding, similarity matching, question answering
    models like BERT(bidirectional encoder representation from transformers)
    RoBERTa, universal sentance enocoder which are pretrained with large text corpora
Finetuning
    updating parameters of pretrainied model using data sets to adapt it to our requirement
    sequence classification, text regression, sequence to sequence modelling
training
    train with specific input output pairs with depp learning procedures
    split data into training and validation
    mini batch training, learning rate schedule, early stopping
evaluation and validation
    evaluate with metrics such cas inception score, mean suqared error
    validate with unseen data
iterative improvement
    experimenting with different hyper parameters, diff models, architectures and preprocessing techniques
    monitor perfomance over time and retrain model periodically
    snapshotting model often
deployment
    model serving
    containerization
    deployment platform
instrastructure
    compute resoufrces, gpus and tpus
    storage
    monitoring and lkogging
    security
CD CI and git

Using Memory based AI model
neural turing machines
memory based models
basically we prepare training data with input output pairs, where kbs, jiras should be enocoded to representations suitable for AI
input should be retrieving information from the kbs, jiras and out is the expected response
so that model understands the relationship between queries and data presented
Inference phase
    new query can eb encoded in the same representation as training data so tat model can use learned knowledge to solve the query

tensor flow
pytorch

training phase
inference phase

2 use cases of cloudera:

1.case relevent info to solution
Input: Concatenate relevant information from the current case (issue description, stack traces, log lines, etc.) 
into a single input sequence.
Output: Solution or response provided for the case.

model architecture
Utilize a sequence-to-sequence (Seq2Seq) model architecture, such as a recurrent neural network (RNN) 
with an encoder-decoder framework or a transformer-based model.
The encoder processes the input sequence and encodes it into a fixed-size context vector.
The decoder takes the context vector as input and generates the solution or response sequence.

Training Objective:
Define a training objective such as sequence generation or sequence-to-sequence learning.
Use teacher forcing during training, where the model is trained to predict the next token in the output sequence given the input sequence and the previous tokens generated by the decoder.

Loss Function:
Utilize loss functions suitable for sequence generation tasks, such as cross-entropy loss or sequence-to-sequence loss.
Compute the loss between the predicted sequence and the ground truth sequence.

Training Process:

Forward Pass:
Pass the input sequence through the encoder to obtain the context vector.
Initialize the decoder with the context vector and an initial token (e.g., <start> token).
Generate the output sequence token by token using the decoder.
Loss Computation:
Compute the loss between the predicted output sequence and the ground truth sequence.
Backpropagation:
Backpropagate the loss through the model to update the parameters using optimization algorithms like
Adam or stochastic gradient descent (SGD).
Repeat:
Repeat the above steps for multiple iterations or epochs until convergence.


2. Remembering Jiras, KBs, Product Docs for Knowledge:

Data Preparation:

Input: Preprocessed and encoded representations of Jiras, KB articles, and product documentation.
Output: No specific output; the model learns to remember and utilize the knowledge during inference.
Model Architecture:

Use memory-augmented neural networks or attention mechanisms to enable the model to read and remember information 
from external memory.
Design the model to store relevant information from Jiras, KB articles, and product docs in memory slots or use
attention mechanisms to focus on important information during inference.
Training Objective:

No specific training objective related to output generation; the model learns to utilize the knowledge during 
training and inference.
Training Process:

Memory Integration:

Integrate the preprocessed and encoded representations of Jiras, KB articles, and product docs into the model's
memory slots or attention mechanisms.
Training:

Train the model using a suitable objective function such as contrastive loss or reconstruction loss.
The objective is to encourage the model to effectively utilize the knowledge stored in memory during inference.
Backpropagation:

Backpropagate the loss through the model to update the parameters, including the memory slots or attention mechanisms.
Iterative Improvement:

Iterate on the training process, adjusting hyperparameters and model architecture as needed to improve performance.
During inference, the model can access the stored knowledge from Jiras, KBs, and product docs to provide contextually
relevant insights or recommendations for solving cases.


A memory-augmented neural network (MANN)


KB Integration:

Input: "How do I troubleshoot network connectivity issues?"
KB Article: "Troubleshooting Network Connectivity"
Training Data: "How do I troubleshoot network connectivity issues? Troubleshooting network connectivity involves 
checking for physical cable connections, verifying IP configurations, and testing network protocols."
In this example, the content of the KB article "Troubleshooting Network Connectivity" is integrated with the input 
question to provide context and knowledge to the model during training.
Jira Integration:

Input: "How do I resolve a NullPointerException in my application?"
Jira Ticket: "NullPointerException Issue 1234"
Training Data: "How do I resolve a NullPointerException in my application? Issue 1234 reported a NullPointerException 
due to a null reference in the application code. To resolve this issue, check for null checks in the affected code and 
handle potential null values appropriately."
Here, the information from the Jira ticket "NullPointerException Issue 1234" is integrated with the input question to 
provide specific details and guidance for resolving the issue.
Product Documentation Integration:

Input: "How do I configure SSL/TLS encryption for my web server?"
Product Documentation: "Web Server Configuration Guide"
Training Data: "How do I configure SSL/TLS encryption for my web server? Refer to the Web Server Configuration Guide 
for step-by-step instructions on enabling SSL/TLS encryption, generating SSL certificates, and configuring encryption protocols."
The content from the product documentation "Web Server Configuration Guide" is integrated with the input question to 
provide detailed instructions and references for configuring SSL/TLS encryption.
Knowledge Fusion:

Input: "What are the best practices for securing a database server?"
KB Article: "Database Security Best Practices"
Jira Ticket: "Security Vulnerability Report 5678"
Product Documentation: "Database Server Administration Guide"
Training Data: "What are the best practices for securing a database server? Review the Database Security Best
Practices KB article, investigate Security Vulnerability Report 5678 for known vulnerabilities, and consult the 
Database Server Administration Guide for configuration recommendations."
This example demonstrates how knowledge from multiple sources, including KB articles, Jira tickets, and product 
documentation, can be fused together to provide comprehensive guidance for the input question.



role od coe is critical for generating the training data
*/